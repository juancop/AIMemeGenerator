{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "stunning-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gorgeous-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/example_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tender-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['template', 'caption_1', 'caption_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hazardous-petersburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>caption_1</th>\n",
       "      <th>caption_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success_kid</td>\n",
       "      <td>Didnt study for a test</td>\n",
       "      <td>still get a higher grade than someone who did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>success_kid</td>\n",
       "      <td>Ate spaghetti with a white shirt on</td>\n",
       "      <td>no stains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>success_kid</td>\n",
       "      <td>New neighbors</td>\n",
       "      <td>Free Wif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awkward_seal</td>\n",
       "      <td>You laugh when your friend says something</td>\n",
       "      <td>He was being serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awkward_seal</td>\n",
       "      <td>took a photo</td>\n",
       "      <td>camera the wrong way</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       template                                   caption_1  \\\n",
       "0   success_kid                      Didnt study for a test   \n",
       "1   success_kid         Ate spaghetti with a white shirt on   \n",
       "2   success_kid                               New neighbors   \n",
       "3  awkward_seal   You laugh when your friend says something   \n",
       "4  awkward_seal                                took a photo   \n",
       "\n",
       "                                        caption_2  \n",
       "0   still get a higher grade than someone who did  \n",
       "1                                       no stains  \n",
       "2                                        Free Wif  \n",
       "3                            He was being serious  \n",
       "4                            camera the wrong way  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "turkish-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_process_column(row):\n",
    "    return row.caption_1 + '|' + row.caption_2 + '|'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "entitled-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['txt'] = data.apply(gen_process_column, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "minus-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_npy(folder, file_name, columns, df):\n",
    "    \"\"\"\n",
    "    Permite crear los archivos de entrenamiento, prueba y validación. \n",
    "    Es necesario definir las columnas que van a ser utilizadas. \n",
    "    \n",
    "    Params\n",
    "    ---------\n",
    "        folder (str):\n",
    "            Ruta donde se van a guardar los archivos.\n",
    "        \n",
    "        file_name (str):\n",
    "            Nombre del archivo de destino.\n",
    "            \n",
    "        columns (list):\n",
    "            Una lista de columnas que contiene la información a exportar\n",
    "            \n",
    "        df (pandas.DataFrame):\n",
    "            Un DataFrame que contenga la información para procesar\n",
    "    \"\"\"\n",
    "    save_path = os.path.join(folder, file_name)\n",
    "    array = df[columns].values\n",
    "    np.save(save_path, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "neutral-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npy('../data', 'prueba.npy', columns = ['template', 'txt'], df = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "measured-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join('../data', 'prueba.npy'), allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "economic-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "label_id_counter = 0\n",
    "for i, row in enumerate(train_data):\n",
    "    template_id = str(row[0]).zfill(12)\n",
    "    text = row[1].lower()\n",
    "    start_index = len(template_id) + 2 + 1 + 2  # template_id, spaces, box_index, spaces\n",
    "    box_index = 0\n",
    "    for j in range(0, len(text)):\n",
    "        char = text[j]\n",
    "        # note: it is critical that the number of spaces plus len(box_index) is >= the convolution width\n",
    "        texts.append(template_id + '  ' + str(box_index) + '  ' + text[0:j])\n",
    "        if char in labels_index:\n",
    "            label_id = labels_index[char]\n",
    "        else:\n",
    "            label_id = label_id_counter\n",
    "            labels_index[char] = label_id\n",
    "            label_id_counter += 1\n",
    "        labels.append(label_id)\n",
    "        if char == '|':\n",
    "            box_index += 1\n",
    "\n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "alpha-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_char_to_int(texts):\n",
    "    char_counts = {}\n",
    "    for text in texts:\n",
    "        for char in text:\n",
    "            char_counts[char] = char_counts[char] + 1 if char in char_counts else 1\n",
    "    char_counts_sorted = sorted(char_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    char_to_int = {}\n",
    "    for i, row in enumerate(char_counts_sorted):\n",
    "        char_to_int[row[0]] = i + 1\n",
    "    return char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "reduced-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = map_char_to_int(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "level-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "studied-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts, char_to_int):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequences.append([char_to_int[char] for char in text])\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "tested-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = texts_to_sequences(texts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "worse-organizer",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_to_int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-516bec413640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-516bec413640>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'char_to_int' is not defined"
     ]
    }
   ],
   "source": [
    "labels = [char_to_int[char] for char in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "southwest-support",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'd': 1,\n",
       " 'i': 2,\n",
       " 'n': 3,\n",
       " 't': 4,\n",
       " 's': 5,\n",
       " 'u': 6,\n",
       " 'y': 7,\n",
       " 'f': 8,\n",
       " 'o': 9,\n",
       " 'r': 10,\n",
       " 'a': 11,\n",
       " 'e': 12,\n",
       " '|': 13,\n",
       " 'l': 14,\n",
       " 'g': 15,\n",
       " 'h': 16,\n",
       " 'm': 17,\n",
       " 'w': 18,\n",
       " 'p': 19,\n",
       " 'b': 20,\n",
       " 'k': 21,\n",
       " 'c': 22,\n",
       " '’': 23}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "discrete-craft",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.strings.unicode_split(example_texts, input_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-floor",
   "metadata": {},
   "source": [
    "# Codificadores de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "weighted-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join('../data', 'prueba.npy'), allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "mature-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(\"\".join(data.txt.values.tolist()).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dense-functionality",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-d6ea0e53bdea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0municode_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.strings.unicode_split(texts, input_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "behind-guitar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0success_kid  0  ',\n",
       " '0success_kid  0   ',\n",
       " '0success_kid  0   d',\n",
       " '0success_kid  0   di',\n",
       " '0success_kid  0   did',\n",
       " '0success_kid  0   didn',\n",
       " '0success_kid  0   didnt',\n",
       " '0success_kid  0   didnt ',\n",
       " '0success_kid  0   didnt s',\n",
       " '0success_kid  0   didnt st',\n",
       " '0success_kid  0   didnt stu',\n",
       " '0success_kid  0   didnt stud',\n",
       " '0success_kid  0   didnt study',\n",
       " '0success_kid  0   didnt study ',\n",
       " '0success_kid  0   didnt study f',\n",
       " '0success_kid  0   didnt study fo',\n",
       " '0success_kid  0   didnt study for',\n",
       " '0success_kid  0   didnt study for ',\n",
       " '0success_kid  0   didnt study for a',\n",
       " '0success_kid  0   didnt study for a ',\n",
       " '0success_kid  0   didnt study for a t',\n",
       " '0success_kid  0   didnt study for a te',\n",
       " '0success_kid  0   didnt study for a tes',\n",
       " '0success_kid  0   didnt study for a test',\n",
       " '0success_kid  1   didnt study for a test|',\n",
       " '0success_kid  1   didnt study for a test| ',\n",
       " '0success_kid  1   didnt study for a test| s',\n",
       " '0success_kid  1   didnt study for a test| st',\n",
       " '0success_kid  1   didnt study for a test| sti',\n",
       " '0success_kid  1   didnt study for a test| stil',\n",
       " '0success_kid  1   didnt study for a test| still',\n",
       " '0success_kid  1   didnt study for a test| still ',\n",
       " '0success_kid  1   didnt study for a test| still g',\n",
       " '0success_kid  1   didnt study for a test| still ge',\n",
       " '0success_kid  1   didnt study for a test| still get',\n",
       " '0success_kid  1   didnt study for a test| still get ',\n",
       " '0success_kid  1   didnt study for a test| still get a',\n",
       " '0success_kid  1   didnt study for a test| still get a ',\n",
       " '0success_kid  1   didnt study for a test| still get a h',\n",
       " '0success_kid  1   didnt study for a test| still get a hi',\n",
       " '0success_kid  1   didnt study for a test| still get a hig',\n",
       " '0success_kid  1   didnt study for a test| still get a high',\n",
       " '0success_kid  1   didnt study for a test| still get a highe',\n",
       " '0success_kid  1   didnt study for a test| still get a higher',\n",
       " '0success_kid  1   didnt study for a test| still get a higher ',\n",
       " '0success_kid  1   didnt study for a test| still get a higher g',\n",
       " '0success_kid  1   didnt study for a test| still get a higher gr',\n",
       " '0success_kid  1   didnt study for a test| still get a higher gra',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grad',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade ',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade t',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade th',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade tha',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than ',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than s',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than so',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than som',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than some',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someo',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someon',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone ',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone w',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone wh',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone who',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone who ',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone who d',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone who di',\n",
       " '0success_kid  1   didnt study for a test| still get a higher grade than someone who did',\n",
       " '0success_kid  0  ',\n",
       " '0success_kid  0   ',\n",
       " '0success_kid  0   a',\n",
       " '0success_kid  0   at',\n",
       " '0success_kid  0   ate',\n",
       " '0success_kid  0   ate ',\n",
       " '0success_kid  0   ate s',\n",
       " '0success_kid  0   ate sp',\n",
       " '0success_kid  0   ate spa',\n",
       " '0success_kid  0   ate spag',\n",
       " '0success_kid  0   ate spagh',\n",
       " '0success_kid  0   ate spaghe',\n",
       " '0success_kid  0   ate spaghet',\n",
       " '0success_kid  0   ate spaghett',\n",
       " '0success_kid  0   ate spaghetti',\n",
       " '0success_kid  0   ate spaghetti ',\n",
       " '0success_kid  0   ate spaghetti w',\n",
       " '0success_kid  0   ate spaghetti wi',\n",
       " '0success_kid  0   ate spaghetti wit',\n",
       " '0success_kid  0   ate spaghetti with',\n",
       " '0success_kid  0   ate spaghetti with ',\n",
       " '0success_kid  0   ate spaghetti with a',\n",
       " '0success_kid  0   ate spaghetti with a ',\n",
       " '0success_kid  0   ate spaghetti with a w',\n",
       " '0success_kid  0   ate spaghetti with a wh',\n",
       " '0success_kid  0   ate spaghetti with a whi',\n",
       " '0success_kid  0   ate spaghetti with a whit',\n",
       " '0success_kid  0   ate spaghetti with a white',\n",
       " '0success_kid  0   ate spaghetti with a white ',\n",
       " '0success_kid  0   ate spaghetti with a white s',\n",
       " '0success_kid  0   ate spaghetti with a white sh',\n",
       " '0success_kid  0   ate spaghetti with a white shi',\n",
       " '0success_kid  0   ate spaghetti with a white shir',\n",
       " '0success_kid  0   ate spaghetti with a white shirt',\n",
       " '0success_kid  0   ate spaghetti with a white shirt ',\n",
       " '0success_kid  0   ate spaghetti with a white shirt o',\n",
       " '0success_kid  0   ate spaghetti with a white shirt on',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on|',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| ',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| n',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| no',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| no ',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| no s',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| no st',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| no sta',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| no stai',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| no stain',\n",
       " '0success_kid  1   ate spaghetti with a white shirt on| no stains',\n",
       " '0success_kid  0  ',\n",
       " '0success_kid  0   ',\n",
       " '0success_kid  0   n',\n",
       " '0success_kid  0   ne',\n",
       " '0success_kid  0   new',\n",
       " '0success_kid  0   new ',\n",
       " '0success_kid  0   new n',\n",
       " '0success_kid  0   new ne',\n",
       " '0success_kid  0   new nei',\n",
       " '0success_kid  0   new neig',\n",
       " '0success_kid  0   new neigh',\n",
       " '0success_kid  0   new neighb',\n",
       " '0success_kid  0   new neighbo',\n",
       " '0success_kid  0   new neighbor',\n",
       " '0success_kid  0   new neighbors',\n",
       " '0success_kid  1   new neighbors|',\n",
       " '0success_kid  1   new neighbors| ',\n",
       " '0success_kid  1   new neighbors| f',\n",
       " '0success_kid  1   new neighbors| fr',\n",
       " '0success_kid  1   new neighbors| fre',\n",
       " '0success_kid  1   new neighbors| free',\n",
       " '0success_kid  1   new neighbors| free ',\n",
       " '0success_kid  1   new neighbors| free w',\n",
       " '0success_kid  1   new neighbors| free wi',\n",
       " '0success_kid  1   new neighbors| free wif',\n",
       " 'awkward_seal  0  ',\n",
       " 'awkward_seal  0   ',\n",
       " 'awkward_seal  0   y',\n",
       " 'awkward_seal  0   yo',\n",
       " 'awkward_seal  0   you',\n",
       " 'awkward_seal  0   you ',\n",
       " 'awkward_seal  0   you l',\n",
       " 'awkward_seal  0   you la',\n",
       " 'awkward_seal  0   you lau',\n",
       " 'awkward_seal  0   you laug',\n",
       " 'awkward_seal  0   you laugh',\n",
       " 'awkward_seal  0   you laugh ',\n",
       " 'awkward_seal  0   you laugh w',\n",
       " 'awkward_seal  0   you laugh wh',\n",
       " 'awkward_seal  0   you laugh whe',\n",
       " 'awkward_seal  0   you laugh when',\n",
       " 'awkward_seal  0   you laugh when ',\n",
       " 'awkward_seal  0   you laugh when y',\n",
       " 'awkward_seal  0   you laugh when yo',\n",
       " 'awkward_seal  0   you laugh when you',\n",
       " 'awkward_seal  0   you laugh when your',\n",
       " 'awkward_seal  0   you laugh when your ',\n",
       " 'awkward_seal  0   you laugh when your f',\n",
       " 'awkward_seal  0   you laugh when your fr',\n",
       " 'awkward_seal  0   you laugh when your fri',\n",
       " 'awkward_seal  0   you laugh when your frie',\n",
       " 'awkward_seal  0   you laugh when your frien',\n",
       " 'awkward_seal  0   you laugh when your friend',\n",
       " 'awkward_seal  0   you laugh when your friend ',\n",
       " 'awkward_seal  0   you laugh when your friend s',\n",
       " 'awkward_seal  0   you laugh when your friend sa',\n",
       " 'awkward_seal  0   you laugh when your friend say',\n",
       " 'awkward_seal  0   you laugh when your friend says',\n",
       " 'awkward_seal  0   you laugh when your friend says ',\n",
       " 'awkward_seal  0   you laugh when your friend says s',\n",
       " 'awkward_seal  0   you laugh when your friend says so',\n",
       " 'awkward_seal  0   you laugh when your friend says som',\n",
       " 'awkward_seal  0   you laugh when your friend says some',\n",
       " 'awkward_seal  0   you laugh when your friend says somet',\n",
       " 'awkward_seal  0   you laugh when your friend says someth',\n",
       " 'awkward_seal  0   you laugh when your friend says somethi',\n",
       " 'awkward_seal  0   you laugh when your friend says somethin',\n",
       " 'awkward_seal  0   you laugh when your friend says something',\n",
       " 'awkward_seal  1   you laugh when your friend says something|',\n",
       " 'awkward_seal  1   you laugh when your friend says something| ',\n",
       " 'awkward_seal  1   you laugh when your friend says something| h',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he ',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he w',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he wa',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was ',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was b',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was be',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was bei',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was bein',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being ',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being s',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being se',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being ser',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being seri',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being serio',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being seriou',\n",
       " 'awkward_seal  1   you laugh when your friend says something| he was being serious',\n",
       " 'awkward_seal  0  ',\n",
       " 'awkward_seal  0   ',\n",
       " 'awkward_seal  0   t',\n",
       " 'awkward_seal  0   to',\n",
       " 'awkward_seal  0   too',\n",
       " 'awkward_seal  0   took',\n",
       " 'awkward_seal  0   took ',\n",
       " 'awkward_seal  0   took a',\n",
       " 'awkward_seal  0   took a ',\n",
       " 'awkward_seal  0   took a p',\n",
       " 'awkward_seal  0   took a ph',\n",
       " 'awkward_seal  0   took a pho',\n",
       " 'awkward_seal  0   took a phot',\n",
       " 'awkward_seal  0   took a photo',\n",
       " 'awkward_seal  1   took a photo|',\n",
       " 'awkward_seal  1   took a photo| ',\n",
       " 'awkward_seal  1   took a photo| c',\n",
       " 'awkward_seal  1   took a photo| ca',\n",
       " 'awkward_seal  1   took a photo| cam',\n",
       " 'awkward_seal  1   took a photo| came',\n",
       " 'awkward_seal  1   took a photo| camer',\n",
       " 'awkward_seal  1   took a photo| camera',\n",
       " 'awkward_seal  1   took a photo| camera ',\n",
       " 'awkward_seal  1   took a photo| camera t',\n",
       " 'awkward_seal  1   took a photo| camera th',\n",
       " 'awkward_seal  1   took a photo| camera the',\n",
       " 'awkward_seal  1   took a photo| camera the ',\n",
       " 'awkward_seal  1   took a photo| camera the w',\n",
       " 'awkward_seal  1   took a photo| camera the wr',\n",
       " 'awkward_seal  1   took a photo| camera the wro',\n",
       " 'awkward_seal  1   took a photo| camera the wron',\n",
       " 'awkward_seal  1   took a photo| camera the wrong',\n",
       " 'awkward_seal  1   took a photo| camera the wrong ',\n",
       " 'awkward_seal  1   took a photo| camera the wrong w',\n",
       " 'awkward_seal  1   took a photo| camera the wrong wa',\n",
       " 'awkward_seal  1   took a photo| camera the wrong way']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-festival",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
