{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections # imports the natural language toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from pylab import rcParams\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/daniel/Documents/GitHub/AIMemeGenerator/data/base_final'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruta = r'/home/daniel/Documents/GitHub/AIMemeGenerator/data/base_final/'\n",
    "os.chdir(ruta)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(752457, 4)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOADING THE DATASET AND SEEING THE DETAILS\n",
    "# If your computer can handle the entire dataset remove the nrows=5000 \n",
    "#data = pd.read_csv('base_final.csv', nrows=5000)\n",
    "data = pd.read_csv('base_final.csv')\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minísculas\n",
    "data.text_1 = data.text_1.str.lower().str.lstrip()\n",
    "data.text_2 = data.text_2.str.lower().str.lstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('spanish')\n",
    "a,b = 'áéíóúüñ','aeiouun'\n",
    "trans = str.maketrans(a,b)\n",
    "stop_words = [x.translate(trans) for x in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crear variable con stopwords removidas, contar palabras con  y sin stopwords\n",
    "\n",
    "n, k = data.shape\n",
    "#range(0,n+1)\n",
    "\n",
    "data[\"text_1_no_stpw\"]     = \"\"\n",
    "data[\"text_2_no_stpw\"]     = \"\"\n",
    "data[\"n_w_text_1\"]         = \"\"\n",
    "data[\"n_w_text_2\"]         = \"\"\n",
    "data[\"n_w_text_1_no_stpw\"] = \"\"\n",
    "data[\"n_w_text_2_no_stpw\"] = \"\"\n",
    "\n",
    "for k in range(0,n):\n",
    "#    x = data.loc[k,\"text_1\"]; y = data.loc[k,\"text_2\"]\n",
    "    word_t1 = nltk.word_tokenize(str(data.loc[k,\"text_1\"]))\n",
    "    word_t2 = nltk.word_tokenize(str(data.loc[k,\"text_2\"]))\n",
    "    data.loc[k,\"n_w_text_1\"] =  len(word_t1)\n",
    "    data.loc[k,\"n_w_text_2\"] =  len(word_t2)\n",
    "    word_t1 = [w for w in word_t1 if not w in stop_words and w.isalpha()]\n",
    "    word_t2 = [w for w in word_t2 if not w in stop_words and w.isalpha()]\n",
    "    data.loc[k,\"n_w_text_1_no_stpw\"] = len(word_t1)\n",
    "    data.loc[k,\"n_w_text_2_no_stpw\"] = len(word_t2)\n",
    "    data.loc[k,\"text_1_no_stpw\"]  = ' '.join([str(elem) for elem in word_t1])\n",
    "    data.loc[k,\"text_2_no_stpw\"]  = ' '.join([str(elem) for elem in word_t2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Con stopwords\n",
    "fig, ax = plt.subplots(1, 2,figsize=(15, 5))\n",
    "sns.boxplot(x=\"plantilla\", y=\"n_w_text_1\", data=data, orient=\"vertical\", showfliers=False, ax = ax[0])\n",
    "ax[0].tick_params('x', labelrotation=90)\n",
    "ax[0].title.set_text('Texto superior')\n",
    "\n",
    "# Sin stopwords\n",
    "sns.boxplot(x=\"plantilla\", y=\"n_w_text_2\", data=data, orient=\"vertical\", showfliers=False, ax = ax[1])\n",
    "ax[1].tick_params('x', labelrotation=90)\n",
    "ax[1].title.set_text('Texto inferior')\n",
    "\n",
    "fig.suptitle(\"Con stopwords\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2,figsize=(15, 5))\n",
    "sns.boxplot(x=\"plantilla\", y=\"n_w_text_1_no_stpw\", data=data, orient=\"vertical\", showfliers=False, ax = ax[0])\n",
    "ax[0].tick_params('x', labelrotation=90)\n",
    "ax[0].title.set_text('Texto superior')\n",
    "\n",
    "# Sin stopwords\n",
    "sns.boxplot(x=\"plantilla\", y=\"n_w_text_2_no_stpw\", data=data, orient=\"vertical\", showfliers=False, ax = ax[1])\n",
    "ax[1].tick_params('x', labelrotation=90)\n",
    "ax[1].title.set_text('Texto inferior')\n",
    "\n",
    "fig.suptitle(\"Sin stopwords\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porcentaje de texto debido a stopwords y distribución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "data[\"text_1_perc_stw\"] = data[\"n_w_text_1_no_stpw\"].divide(data[\"n_w_text_1\"]replace(0,np.nan))\n",
    "data[\"text_2_perc_stw\"] = data[\"n_w_text_2_no_stpw\"].divide(data[\"n_w_text_2\"]replace(0,np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2,figsize=(15, 5))\n",
    "sns.boxplot(x=\"plantilla\", y=\"text_1_perc_stw\", data=data, orient=\"vertical\", showfliers=False, ax = ax[0])\n",
    "ax[0].tick_params('x', labelrotation=90)\n",
    "ax[0].title.set_text('Texto superior')\n",
    "\n",
    "# Sin stopwords\n",
    "sns.boxplot(x=\"plantilla\", y=\"text_2_perc_stw\", data=data, orient=\"vertical\", showfliers=False, ax = ax[1])\n",
    "ax[1].tick_params('x', labelrotation=90)\n",
    "ax[1].title.set_text('Texto inferior')\n",
    "\n",
    "fig.suptitle(\"Distribución de porcentaje de stopwords en texto de memes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proporción de palabras entre texto superior y texto inferior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rel_text_1_2\"] = data[\"n_w_text_1\"].divide(data[\"n_w_text_2\"].replace(0,np.nan))\n",
    "data[\"rel_text_1_2_no_stpw\"] =  data[\"n_w_text_1_no_stpw\"].divide(data[\"n_w_text_2_no_stpw\"].replace(0,np.nan))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2,figsize=(15, 5))\n",
    "sns.boxplot(x=\"plantilla\", y=\"rel_text_1_2\", data=data, orient=\"vertical\", showfliers=False, ax = ax[0])\n",
    "ax[0].tick_params('x', labelrotation=90)\n",
    "ax[0].title.set_text('Con stopwords')\n",
    "\n",
    "# Sin stopwords\n",
    "sns.boxplot(x=\"plantilla\", y=\"rel_text_1_2_no_stpw\", data=data, orient=\"vertical\", showfliers=False, ax = ax[1])\n",
    "ax[1].tick_params('x', labelrotation=90)\n",
    "ax[1].title.set_text('Sin stopwords')\n",
    "\n",
    "fig.suptitle(\"Relación entre texto superior y texto inferior\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Función para calcular top n k-gramas\n",
    "\n",
    "# Following code grabbed from:\n",
    "# https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "# we will use it in our context to create some visualizations.\n",
    "def get_top_n_words(data,text_label,n=1,k=1,stopwords=None):\n",
    "    template = data[\"plantilla\"].unique()\n",
    "    template_top_n_k_gram = {}\n",
    "    for t in template:\n",
    "        corpus = data[data[\"plantilla\"]==t][text_label]\n",
    "        corpus.dropna(inplace=True)\n",
    "        #if stopwords is not None:\n",
    "        vec = CountVectorizer(ngram_range=(k,k),stop_words = stopwords).fit(corpus)\n",
    "        #else:\n",
    "        #    vec = CountVectorizer(ngram_range=(k,k)).fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        template_top_n_k_gram[t] = words_freq[:n]\n",
    "    return template_top_n_k_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gramas (top 20)\n",
    "common_words_1       = get_top_n_words(data,\"text_1\", 20,1)\n",
    "common_words_1_no_stpw = get_top_n_words(data,\"text_1\", 20,1,stopwords=stop_words)\n",
    "\n",
    "# 2-gramas (top 20)\n",
    "common_words_2       = get_top_n_words(data,\"text_1\", 20,2)\n",
    "common_words_2_no_stpw = get_top_n_words(data,\"text_1\", 20,2,stopwords=stop_words)\n",
    "\n",
    "# 3-gramas (top 20)\n",
    "common_words_3       = get_top_n_words(data,\"text_1\", 20,3)\n",
    "common_words_3_no_stpw = get_top_n_words(data,\"text_1\", 20,3,stopwords=stop_words)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
