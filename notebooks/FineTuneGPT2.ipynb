{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FineTuneGPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "source": [
        "\n",
        "# Entrenamiento del Modelo GPT-2 \n",
        "\n",
        "El objetivo de este notebook es llevar acabo el *fine-tuning* del modelo GPT-2 de generación de texto en español, para que se desempeñe en la generación de texto para memes. El modelo GPT-2 se encuentra en almacenado en [la página de Hugging Face](https://huggingface.co/datificate/gpt2-small-spanish). \n",
        "\n",
        "\n",
        "Para ello es necesario descargar las librerías presentes en el archivo ``requirements.txt``\n",
        "\n",
        "A continuación especificamos el contenido del notebook.\n",
        "\n",
        "1. Instalación de librerías\n",
        "2. Carga de Información\n",
        "3. Preparación de Texto\n",
        "4. Separación en Entrenamiento y Prueba\n",
        "5. Entrenamiento del Modelo\n",
        "6. Ejemplos de Texto Generado\n",
        "\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Instalación de Librerías"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnBIWT1Q71Iv",
        "outputId": "ce7945a9-652e-40b3-d4c4-291c87b45340"
      },
      "source": [
        "!cd \"drive/MyDrive/Retrain\" && git clone https://github.com/juancop/AIMemeGenerator # Se clona el repo porque se trabaja desde Google Colab"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: cd: drive/MyDrive/Retrain: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrPp2Gag98h2",
        "outputId": "961283d4-6870-4e0c-87f8-705c6a97f61d"
      },
      "source": [
        "!cd \"drive/MyDrive/Retrain/AIMemeGenerator\" && pip install -r requirements.txt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/18/374af421dfbe74379a458e58ab40cf46b35c3206ce8e183e28c1c627494d/tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 53kB/s \n",
            "\u001b[?25hCollecting transformers==4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n",
            "Collecting pandas==1.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/51/48f3fc47c4e2144da2806dfb6629c4dd1fa3d5a143f9652b141e979a8ca9/pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9MB 50.6MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (1.6.3)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 1)) (1.12.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 2)) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 2)) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 2)) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->-r requirements.txt (line 4)) (2018.9)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow==2.3.1->-r requirements.txt (line 1)) (56.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (1.28.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 1)) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 2.3.1 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorflow, sacremoses, tokenizers, transformers, pandas, threadpoolctl, scikit-learn\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed pandas-1.2.4 sacremoses-0.0.45 scikit-learn-0.24.1 tensorflow-2.3.1 tensorflow-estimator-2.3.0 threadpoolctl-2.1.0 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "source": [
        "La librería de transformers debe ser instalada directamente desde el repositorio"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnhJfELO_lsr",
        "outputId": "1a854683-af34-47d2-d710-9847411a7117"
      },
      "source": [
        "! cd \"/content/drive/MyDrive/Retrain/AIMemeGenerator/transformers\" && git clone https://github.com/huggingface/transformers "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/82)\u001b[K\rremote: Counting objects:   2% (2/82)\u001b[K\rremote: Counting objects:   3% (3/82)\u001b[K\rremote: Counting objects:   4% (4/82)\u001b[K\rremote: Counting objects:   6% (5/82)\u001b[K\rremote: Counting objects:   7% (6/82)\u001b[K\rremote: Counting objects:   8% (7/82)\u001b[K\rremote: Counting objects:   9% (8/82)\u001b[K\rremote: Counting objects:  10% (9/82)\u001b[K\rremote: Counting objects:  12% (10/82)\u001b[K\rremote: Counting objects:  13% (11/82)\u001b[K\rremote: Counting objects:  14% (12/82)\u001b[K\rremote: Counting objects:  15% (13/82)\u001b[K\rremote: Counting objects:  17% (14/82)\u001b[K\rremote: Counting objects:  18% (15/82)\u001b[K\rremote: Counting objects:  19% (16/82)\u001b[K\rremote: Counting objects:  20% (17/82)\u001b[K\rremote: Counting objects:  21% (18/82)\u001b[K\rremote: Counting objects:  23% (19/82)\u001b[K\rremote: Counting objects:  24% (20/82)\u001b[K\rremote: Counting objects:  25% (21/82)\u001b[K\rremote: Counting objects:  26% (22/82)\u001b[K\rremote: Counting objects:  28% (23/82)\u001b[K\rremote: Counting objects:  29% (24/82)\u001b[K\rremote: Counting objects:  30% (25/82)\u001b[K\rremote: Counting objects:  31% (26/82)\u001b[K\rremote: Counting objects:  32% (27/82)\u001b[K\rremote: Counting objects:  34% (28/82)\u001b[K\rremote: Counting objects:  35% (29/82)\u001b[K\rremote: Counting objects:  36% (30/82)\u001b[K\rremote: Counting objects:  37% (31/82)\u001b[K\rremote: Counting objects:  39% (32/82)\u001b[K\rremote: Counting objects:  40% (33/82)\u001b[K\rremote: Counting objects:  41% (34/82)\u001b[K\rremote: Counting objects:  42% (35/82)\u001b[K\rremote: Counting objects:  43% (36/82)\u001b[K\rremote: Counting objects:  45% (37/82)\u001b[K\rremote: Counting objects:  46% (38/82)\u001b[K\rremote: Counting objects:  47% (39/82)\u001b[K\rremote: Counting objects:  48% (40/82)\u001b[K\rremote: Counting objects:  50% (41/82)\u001b[K\rremote: Counting objects:  51% (42/82)\u001b[K\rremote: Counting objects:  52% (43/82)\u001b[K\rremote: Counting objects:  53% (44/82)\u001b[K\rremote: Counting objects:  54% (45/82)\u001b[K\rremote: Counting objects:  56% (46/82)\u001b[K\rremote: Counting objects:  57% (47/82)\u001b[K\rremote: Counting objects:  58% (48/82)\u001b[K\rremote: Counting objects:  59% (49/82)\u001b[K\rremote: Counting objects:  60% (50/82)\u001b[K\rremote: Counting objects:  62% (51/82)\u001b[K\rremote: Counting objects:  63% (52/82)\u001b[K\rremote: Counting objects:  64% (53/82)\u001b[K\rremote: Counting objects:  65% (54/82)\u001b[K\rremote: Counting objects:  67% (55/82)\u001b[K\rremote: Counting objects:  68% (56/82)\u001b[K\rremote: Counting objects:  69% (57/82)\u001b[K\rremote: Counting objects:  70% (58/82)\u001b[K\rremote: Counting objects:  71% (59/82)\u001b[K\rremote: Counting objects:  73% (60/82)\u001b[K\rremote: Counting objects:  74% (61/82)\u001b[K\rremote: Counting objects:  75% (62/82)\u001b[K\rremote: Counting objects:  76% (63/82)\u001b[K\rremote: Counting objects:  78% (64/82)\u001b[K\rremote: Counting objects:  79% (65/82)\u001b[K\rremote: Counting objects:  80% (66/82)\u001b[K\rremote: Counting objects:  81% (67/82)\u001b[K\rremote: Counting objects:  82% (68/82)\u001b[K\rremote: Counting objects:  84% (69/82)\u001b[K\rremote: Counting objects:  85% (70/82)\u001b[K\rremote: Counting objects:  86% (71/82)\u001b[K\rremote: Counting objects:  87% (72/82)\u001b[K\rremote: Counting objects:  89% (73/82)\u001b[K\rremote: Counting objects:  90% (74/82)\u001b[K\rremote: Counting objects:  91% (75/82)\u001b[K\rremote: Counting objects:  92% (76/82)\u001b[K\rremote: Counting objects:  93% (77/82)\u001b[K\rremote: Counting objects:  95% (78/82)\u001b[K\rremote: Counting objects:  96% (79/82)\u001b[K\rremote: Counting objects:  97% (80/82)\u001b[K\rremote: Counting objects:  98% (81/82)\u001b[K\rremote: Counting objects: 100% (82/82)\u001b[K\rremote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/35)\u001b[K\rremote: Compressing objects:   5% (2/35)\u001b[K\rremote: Compressing objects:   8% (3/35)\u001b[K\rremote: Compressing objects:  11% (4/35)\u001b[K\rremote: Compressing objects:  14% (5/35)\u001b[K\rremote: Compressing objects:  17% (6/35)\u001b[K\rremote: Compressing objects:  20% (7/35)\u001b[K\rremote: Compressing objects:  22% (8/35)\u001b[K\rremote: Compressing objects:  25% (9/35)\u001b[K\rremote: Compressing objects:  28% (10/35)\u001b[K\rremote: Compressing objects:  31% (11/35)\u001b[K\rremote: Compressing objects:  34% (12/35)\u001b[K\rremote: Compressing objects:  37% (13/35)\u001b[K\rremote: Compressing objects:  40% (14/35)\u001b[K\rremote: Compressing objects:  42% (15/35)\u001b[K\rremote: Compressing objects:  45% (16/35)\u001b[K\rremote: Compressing objects:  48% (17/35)\u001b[K\rremote: Compressing objects:  51% (18/35)\u001b[K\rremote: Compressing objects:  54% (19/35)\u001b[K\rremote: Compressing objects:  57% (20/35)\u001b[K\rremote: Compressing objects:  60% (21/35)\u001b[K\rremote: Compressing objects:  62% (22/35)\u001b[K\rremote: Compressing objects:  65% (23/35)\u001b[K\rremote: Compressing objects:  68% (24/35)\u001b[K\rremote: Compressing objects:  71% (25/35)\u001b[K\rremote: Compressing objects:  74% (26/35)\u001b[K\rremote: Compressing objects:  77% (27/35)\u001b[K\rremote: Compressing objects:  80% (28/35)\u001b[K\rremote: Compressing objects:  82% (29/35)\u001b[K\rremote: Compressing objects:  85% (30/35)\u001b[K\rremote: Compressing objects:  88% (31/35)\u001b[K\rremote: Compressing objects:  91% (32/35)\u001b[K\rremote: Compressing objects:  94% (33/35)\u001b[K\rremote: Compressing objects:  97% (34/35)\u001b[K\rremote: Compressing objects: 100% (35/35)\u001b[K\rremote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 82 (delta 56), reused 64 (delta 41), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (82/82), done.\n",
            "From https://github.com/huggingface/transformers\n",
            "   9046ff261..9f6743974  multi_regress     -> origin/multi_regress\n",
            "   ed2331e68..38e841b84  new_training_tuto -> origin/new_training_tuto\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bOlDQZAkCCvQ",
        "outputId": "6bc66d92-0819-4346-b371-d634306e79a7"
      },
      "source": [
        "!pip install -U pandas-profiling"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pandas-profiling\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/12/e2870750c5320116efe7bebd4ae1709cd7e35e3bc23ac8039864b05b9497/pandas_profiling-2.11.0-py2.py3-none-any.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.2MB/s \n",
            "\u001b[?25hCollecting requests>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hCollecting phik>=0.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/ce/193e8ddf62d4be643b9b4b20e8e9c63b2f6a20f92778c0410c629f89bdaa/phik-0.11.2.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 11.6MB/s \n",
            "\u001b[?25hCollecting tangled-up-in-unicode>=0.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d1/58bfbe263494741a47140049b989ad42a8941854e8d34f1af90640c6c9f9/tangled_up_in_unicode-0.0.7-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 28.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.19.5)\n",
            "Collecting confuse>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/55/b4726d81e5d6509fa3441f770f8a9524612627dc1b2a7d6209d1d20083fe/confuse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.2.4)\n",
            "Collecting visions[type_image_path]==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/30/b1e70bc55962239c4c3c9660e892be2d8247a882135a3035c10ff7f02cde/visions-0.6.0-py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (20.3.0)\n",
            "Collecting htmlmin>=0.1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
            "Collecting tqdm>=4.48.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: ipywidgets>=7.5.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (7.6.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from confuse>=1.0.0->pandas-profiling) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.6.0->pandas-profiling) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"type_image_path\" in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.6.0->pandas-profiling) (7.1.2)\n",
            "Collecting imagehash; extra == \"type_image_path\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/18/9dbb772b5ef73a3069c66bb5bf29b9fb4dd57af0d5790c781c3f559bcca6/ImageHash-4.2.0-py2.py3-none-any.whl (295kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 58.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling) (4.10.1)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling) (5.0.5)\n",
            "Requirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas-profiling) (5.1.3)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->pandas-profiling) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.2.0->pandas-profiling) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.4->visions[type_image_path]==0.6.0->pandas-profiling) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.6.0->pandas-profiling) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling) (5.3.5)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->pandas-profiling) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling) (4.7.1)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling) (22.0.3)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (5.6.1)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (20.9)\n",
            "Building wheels for collected packages: phik, htmlmin\n",
            "  Building wheel for phik (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phik: filename=phik-0.11.2-cp37-none-any.whl size=1107413 sha256=084d17c27d8b78012d3b1f8a4ace6aa81926c146c42b96d9bdfa1819789e4070\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/a3/b0/f27b1cfe32ea131a3715169132ff6d85653789e80e966c3bf6\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-cp37-none-any.whl size=27085 sha256=6fb01c4ae0a354c9c0a79cb35f0e0a190ded1867730ba86fc02b278bac851fbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
            "Successfully built phik htmlmin\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datasets 1.6.2 has requirement tqdm<4.50.0,>=4.27, but you'll have tqdm 4.60.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: phik 0.11.2 has requirement scipy>=1.5.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: requests, phik, tangled-up-in-unicode, confuse, imagehash, visions, htmlmin, tqdm, pandas-profiling\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "Successfully installed confuse-1.4.0 htmlmin-0.1.12 imagehash-4.2.0 pandas-profiling-2.11.0 phik-0.11.2 requests-2.25.1 tangled-up-in-unicode-0.0.7 tqdm-4.60.0 visions-0.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFbGqeFbHJXB",
        "outputId": "cb8be017-73c4-48ac-92f4-b49c46ce469d"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 14.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 204kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.2.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 16.8MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, fsspec, huggingface-hub, datasets\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCzCEDzEHZfd",
        "outputId": "5d2ac044-73e7-4501-ed70-9764f89888f5"
      },
      "source": [
        "! pip install git+git://github.com/huggingface/transformers/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/huggingface/transformers/\n",
            "  Cloning git://github.com/huggingface/transformers/ to /tmp/pip-req-build-gkkdmosi\n",
            "  Running command git clone -q git://github.com/huggingface/transformers/ /tmp/pip-req-build-gkkdmosi\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2168795 sha256=df97380b4a7a33c60c013730a0fe250e4133a16e2b866636415c4a71de8b24a7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fc_62p5e/wheels/dc/e5/1e/3a2977a646558fca07585cadcf56aa4a910e995ba945961c4e\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.5.1\n",
            "    Uninstalling transformers-4.5.1:\n",
            "      Successfully uninstalled transformers-4.5.1\n",
            "Successfully installed transformers-4.6.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2LGEbet9tPc"
      },
      "source": [
        "# 2. Carga de Información\n",
        "En esta etapa cargaremos la información necesaria para el entrenamiento del modelo. En particular, se requiere la base de datos limpia y con filtros de groserías, ``base_final.csv``. Los textos de esta base de datos no contienen groserías, y serán limpiados para eliminar todos los caracteres que no sean alfanuméricos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDVEZ-zO72Ql"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRBnqzJ_-vKz"
      },
      "source": [
        "drive_path = 'drive/MyDrive/Retrain/AIMemeGenerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKz7UFT7-pQc"
      },
      "source": [
        "df = pd.read_csv(os.path.join(drive_path, 'data/base_final/base_final.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxw68Gka-79_",
        "outputId": "6780b721-739e-42ac-c2e5-ba43d363b0d1"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id_imagen', 'meme_id', 'plantilla', 'texto_no_tilde', 'text_1',\n",
              "       'text_2'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji24mj_h-8d_"
      },
      "source": [
        "df = df[(~df.text_1.isna()) & (~df.text_2.isna())]\n",
        "df[\"text_1\"] = df[\"text_1\"].str.lower()\n",
        "df[\"text_2\"] = df[\"text_2\"].str.lower()\n",
        "df['meme_id'] = df['meme_id'].apply('{:0>4}'.format)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXNq9sWUzE_n",
        "outputId": "f8d20298-6fd7-49d4-8ce8-091a4223b34c"
      },
      "source": [
        "df = df[~(df.text_1.str.contains('\\|') & df.text_2.str.contains('\\|'))]\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "676777"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkRg55mk-Puz"
      },
      "source": [
        "df['text_1'] = df.text_1.str.replace(r'[^A-Za-z0-9 ?¿!¡]+', r'')\n",
        "df['text_2'] = df.text_2.str.replace(r'[^A-Za-z0-9 ?¿!¡]+', r'')\n",
        "\n",
        "df[\"text_1\"] = df[\"text_1\"].str.replace('\\s+', ' ', regex=True)\n",
        "df['text_2'] = df[\"text_2\"].str.replace('\\s+', ' ', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# 3. Preparación de Texto\n",
        "\n",
        "Para el entrenamiento del modelo, se requiere tokenizar una única frase, por lo que debemos volver a unir los textos superior e inferior, con un detalle adicional... ¡Indicaremos en qué parte se dividen! De esta forma el modelo no sólo será capaz de generar texto de memes, sino que nos indicará en qué parte debemos separarlo. \n",
        "\n",
        "De esta forma, utilizamos el caracter `|` para dividirlo. \n",
        "\n",
        "Adicionalmente, a cada frase se le va a añadir un token que refleje el id del meme al que pertenece el texto. Así, podremos condicionar el texto generado a partir de un token. Por ejemplo, para el meme asociado a la plantilla 001, el token inicial sería, `<|001|>`."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x30KP0YUA3oH"
      },
      "source": [
        "def GPT_text(row):\n",
        "    meme_id = row.meme_id\n",
        "    text_1 = row.text_1 \n",
        "    text_2 = row.text_2 \n",
        "    return '<|'+meme_id+'|>' + text_1.strip() + ' | ' + text_2.strip()\n",
        "\n",
        "df['GPT_text'] = df.apply(GPT_text, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFiedk1nCQs3"
      },
      "source": [
        "df = df.drop_duplicates(subset=['GPT_text', 'meme_id', 'text_1', 'text_2'], keep='last')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWnO0j4_W_a6",
        "outputId": "fd461497-98e4-47d4-e893-70af57807d1c"
      },
      "source": [
        "df.meme_id.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0004    208152\n",
              "0003    167777\n",
              "0012    126011\n",
              "0007     59834\n",
              "0011     39244\n",
              "0008     20869\n",
              "0006     17993\n",
              "0013     11293\n",
              "0005      6086\n",
              "0010      3049\n",
              "Name: meme_id, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iROXYF1GW1tO"
      },
      "source": [
        "df = df.groupby('meme_id').head(20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nw2B3Sq-0vi",
        "outputId": "cf9b6e51-3276-4911-9282-9c95e6c15d1f"
      },
      "source": [
        "df.meme_id.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0003    20000\n",
              "0007    20000\n",
              "0011    20000\n",
              "0012    20000\n",
              "0008    20000\n",
              "0004    20000\n",
              "0006    17993\n",
              "0013    11293\n",
              "0005     6086\n",
              "0010     3049\n",
              "Name: meme_id, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "source": [
        "# 4. Separación en Entrenamiento y Prueba\n",
        "\n",
        "Al igual que en cualquier otro modelo, es necesario dividir nuestra muestra en entrenamiento y validación. En este paso tomamos una muestra aletoria de tamaño 10% para ser el test, y el 90% restante será el train. El objetivo de esto es realizar el finetuning sin hacer overfitting y dañar el desempeño del modelo."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5UXR_YbA8u4"
      },
      "source": [
        "train_text, test_text, train_id, test_id  = train_test_split(df['GPT_text'], df['meme_id'], train_size=.9, random_state=42) # Para hacerlo estratificado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTeD6K9JA_wT",
        "outputId": "887bb7f7-5359-44eb-ef49-0b3ffbbd4650"
      },
      "source": [
        "print(f'Memes en Train: {len(train_text)}\\nMemes en Test: {len(test_text)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memes en Train: 142578\n",
            "Memes en Test: 15843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1-UD7GOBBf4"
      },
      "source": [
        "model_data_path = os.path.join(drive_path, 'model_data')\n",
        "\n",
        "train = os.path.join(model_data_path, '20k_sample_train_split.txt')\n",
        "test = os.path.join(model_data_path, '20k_sample_test_split.txt')\n",
        "\n",
        "with open(train, 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(train_text.values.tolist()))\n",
        "\n",
        "with open(test, 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(test_text.values.tolist()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujAZ_qEsBZdq"
      },
      "source": [
        "# 5. Entrenamiento de Modelo\n",
        "\n",
        "En esta sección utilizaremos ya los elementos de texto generados para entrenar el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DDcQT0TGLMg"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/GPT2/AIMemeGenerator/transformers/examples/\")\n",
        "os.chdir(\"./language-modeling\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w83MV9NRBgZ6",
        "outputId": "c9c74a60-3efe-4706-8071-9adc0aeec0ae"
      },
      "source": [
        "!python run_clm.py \\\n",
        "--model_type 'datificate/gpt2-small-spanish' \\\n",
        "--model_name_or_path 'datificate/gpt2-small-spanish' \\\n",
        "--train_file '/content/drive/MyDrive/Retrain/AIMemeGenerator/model_data/20k_sample_train_split.txt' \\\n",
        "--do_train \\\n",
        "--validation_file '/content/drive/MyDrive/Retrain/AIMemeGenerator/model_data/20k_sample_test_split.txt' \\\n",
        "--do_eval \\\n",
        "--per_device_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 3 \\\n",
        "--fp16 \\\n",
        "--output_dir='/content/drive/MyDrive/Retrain/AIMemeGenerator/final_model/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-03 16:31:54.431290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "05/03/2021 16:31:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "05/03/2021 16:31:56 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/Retrain/AIMemeGenerator/final_model/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May03_16-31-56_c0307c08c44c, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/Retrain/AIMemeGenerator/final_model/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
            "05/03/2021 16:31:56 - WARNING - datasets.builder -   Using custom data configuration default-940f0639ebaf701c\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-940f0639ebaf701c/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-940f0639ebaf701c/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1530] 2021-05-03 16:31:56,786 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6i506tbn\n",
            "Downloading: 100% 817/817 [00:00<00:00, 814kB/s]\n",
            "[INFO|file_utils.py:1534] 2021-05-03 16:31:56,992 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|file_utils.py:1537] 2021-05-03 16:31:56,992 >> creating metadata file for /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:500] 2021-05-03 16:31:56,992 >> loading configuration file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:536] 2021-05-03 16:31:56,993 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:500] 2021-05-03 16:31:57,198 >> loading configuration file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:536] 2021-05-03 16:31:57,199 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1530] 2021-05-03 16:31:57,412 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmwjvu6gs\n",
            "Downloading: 100% 850k/850k [00:00<00:00, 2.62MB/s]\n",
            "[INFO|file_utils.py:1534] 2021-05-03 16:31:57,947 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "[INFO|file_utils.py:1537] 2021-05-03 16:31:57,947 >> creating metadata file for /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "[INFO|file_utils.py:1530] 2021-05-03 16:31:58,157 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6fhk3ln_\n",
            "Downloading: 100% 508k/508k [00:00<00:00, 1.58MB/s]\n",
            "[INFO|file_utils.py:1534] 2021-05-03 16:31:58,687 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "[INFO|file_utils.py:1537] 2021-05-03 16:31:58,687 >> creating metadata file for /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "[INFO|file_utils.py:1530] 2021-05-03 16:31:59,312 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjypbjgjc\n",
            "Downloading: 100% 387/387 [00:00<00:00, 353kB/s]\n",
            "[INFO|file_utils.py:1534] 2021-05-03 16:31:59,519 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "[INFO|file_utils.py:1537] 2021-05-03 16:31:59,519 >> creating metadata file for /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "[INFO|file_utils.py:1530] 2021-05-03 16:31:59,724 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppxs0fnmn\n",
            "Downloading: 100% 620/620 [00:00<00:00, 593kB/s]\n",
            "[INFO|file_utils.py:1534] 2021-05-03 16:31:59,939 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "[INFO|file_utils.py:1537] 2021-05-03 16:31:59,939 >> creating metadata file for /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-03 16:31:59,939 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-03 16:31:59,939 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-03 16:31:59,939 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-03 16:31:59,939 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-03 16:31:59,939 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-03 16:31:59,939 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "[INFO|file_utils.py:1530] 2021-05-03 16:32:00,274 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8mszjxw6\n",
            "Downloading: 100% 510M/510M [00:07<00:00, 66.1MB/s]\n",
            "[INFO|file_utils.py:1534] 2021-05-03 16:32:08,409 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "[INFO|file_utils.py:1537] 2021-05-03 16:32:08,409 >> creating metadata file for /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "[INFO|modeling_utils.py:1113] 2021-05-03 16:32:08,409 >> loading weights file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "[INFO|modeling_utils.py:1254] 2021-05-03 16:32:13,190 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1263] 2021-05-03 16:32:13,190 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at datificate/gpt2-small-spanish.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 1/1 [00:09<00:00,  9.74s/ba]\n",
            "100% 1/1 [00:00<00:00,  1.09ba/s]\n",
            "05/03/2021 16:32:23 - WARNING - __main__ -   The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.\n",
            "100% 1/1 [00:02<00:00,  2.75s/ba]\n",
            "100% 1/1 [00:00<00:00,  3.36ba/s]\n",
            "[INFO|trainer.py:411] 2021-05-03 16:32:38,391 >> Using amp fp16 backend\n",
            "[INFO|trainer.py:1130] 2021-05-03 16:32:38,560 >> ***** Running training *****\n",
            "[INFO|trainer.py:1131] 2021-05-03 16:32:38,561 >>   Num examples = 3374\n",
            "[INFO|trainer.py:1132] 2021-05-03 16:32:38,561 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1133] 2021-05-03 16:32:38,561 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1134] 2021-05-03 16:32:38,561 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1135] 2021-05-03 16:32:38,561 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1136] 2021-05-03 16:32:38,561 >>   Total optimization steps = 10122\n",
            "{'loss': 3.2952, 'learning_rate': 4.753013238490417e-05, 'epoch': 0.15}\n",
            "{'loss': 2.9421, 'learning_rate': 4.5060264769808345e-05, 'epoch': 0.3}\n",
            "{'loss': 2.8526, 'learning_rate': 4.259039715471251e-05, 'epoch': 0.44}\n",
            "{'loss': 2.7968, 'learning_rate': 4.012052953961668e-05, 'epoch': 0.59}\n",
            "{'loss': 2.752, 'learning_rate': 3.7650661924520845e-05, 'epoch': 0.74}\n",
            "{'loss': 2.7126, 'learning_rate': 3.518079430942502e-05, 'epoch': 0.89}\n",
            "{'loss': 2.6758, 'learning_rate': 3.271586642955938e-05, 'epoch': 1.04}\n",
            "{'loss': 2.6042, 'learning_rate': 3.0245998814463544e-05, 'epoch': 1.19}\n",
            "{'loss': 2.5857, 'learning_rate': 2.7776131199367715e-05, 'epoch': 1.33}\n",
            "{'loss': 2.5757, 'learning_rate': 2.5306263584271883e-05, 'epoch': 1.48}\n",
            "{'loss': 2.5641, 'learning_rate': 2.2836395969176054e-05, 'epoch': 1.63}\n",
            "{'loss': 2.557, 'learning_rate': 2.0371468089310414e-05, 'epoch': 1.78}\n",
            "{'loss': 2.5516, 'learning_rate': 1.7906540209444775e-05, 'epoch': 1.93}\n",
            "{'loss': 2.5156, 'learning_rate': 1.5436672594348946e-05, 'epoch': 2.07}\n",
            "{'loss': 2.4827, 'learning_rate': 1.2966804979253113e-05, 'epoch': 2.22}\n",
            "{'loss': 2.4807, 'learning_rate': 1.0496937364157281e-05, 'epoch': 2.37}\n",
            "{'loss': 2.476, 'learning_rate': 8.02706974906145e-06, 'epoch': 2.52}\n",
            "{'loss': 2.478, 'learning_rate': 5.557202133965619e-06, 'epoch': 2.67}\n",
            "{'loss': 2.4671, 'learning_rate': 3.087334518869789e-06, 'epoch': 2.82}\n",
            "{'loss': 2.4682, 'learning_rate': 6.174669037739577e-07, 'epoch': 2.96}\n",
            "100% 10122/10122 [50:49<00:00,  3.26it/s][INFO|trainer.py:1324] 2021-05-03 17:23:28,507 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3049.9465, 'train_samples_per_second': 3.319, 'epoch': 3.0}\n",
            "100% 10122/10122 [50:49<00:00,  3.32it/s]\n",
            "[INFO|trainer.py:1790] 2021-05-03 17:23:28,688 >> Saving model checkpoint to /content/drive/MyDrive/Retrain/AIMemeGenerator/final_model/\n",
            "[INFO|configuration_utils.py:334] 2021-05-03 17:23:28,693 >> Configuration saved in /content/drive/MyDrive/Retrain/AIMemeGenerator/final_model/config.json\n",
            "[INFO|modeling_utils.py:858] 2021-05-03 17:23:31,102 >> Model weights saved in /content/drive/MyDrive/Retrain/AIMemeGenerator/final_model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-03 17:23:31,107 >> tokenizer config file saved in /content/drive/MyDrive/Retrain/AIMemeGenerator/final_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-03 17:23:31,111 >> Special tokens file saved in /content/drive/MyDrive/Retrain/AIMemeGenerator/final_model/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:905] 2021-05-03 17:23:31,234 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   epoch                      =        3.0\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   init_mem_cpu_alloc_delta   =     1026MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   init_mem_gpu_alloc_delta   =      487MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   train_mem_cpu_alloc_delta  =       11MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   train_mem_gpu_alloc_delta  =     1435MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   train_mem_gpu_peaked_delta =     3220MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   train_runtime              = 0:50:49.94\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   train_samples              =       3374\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:23:31,234 >>   train_samples_per_second   =      3.319\n",
            "05/03/2021 17:23:31 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:2012] 2021-05-03 17:23:31,352 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2014] 2021-05-03 17:23:31,352 >>   Num examples = 374\n",
            "[INFO|trainer.py:2017] 2021-05-03 17:23:31,352 >>   Batch size = 8\n",
            "100% 47/47 [00:50<00:00,  1.07s/it]\n",
            "[INFO|trainer_pt_utils.py:905] 2021-05-03 17:24:21,665 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,665 >>   epoch                     =        3.0\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,665 >>   eval_loss                 =     2.5015\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,665 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,665 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,665 >>   eval_mem_gpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,666 >>   eval_mem_gpu_peaked_delta =     5596MB\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,666 >>   eval_runtime              = 0:00:50.19\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,666 >>   eval_samples              =        374\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,666 >>   eval_samples_per_second   =      7.452\n",
            "[INFO|trainer_pt_utils.py:910] 2021-05-03 17:24:21,666 >>   perplexity                =    12.2005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fktes187Gi-o",
        "outputId": "75ff2065-af43-4b30-cc0e-0d3756a48413"
      },
      "source": [
        "# setup imports to use the model\n",
        "model = TFGPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/GPT2/AIMemeGenerator/saved_model3/', from_pt=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/drive/MyDrive/GPT2/AIMemeGenerator/saved_model3/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.4.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'lm_head.weight', 'transformer.h.3.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.5.attn.masked_bias']\n",
            "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "source": [
        "# 6. Algunos Ejemplos de Generación de texto (sin procesar)\n",
        "\n",
        "En este punto se prueban diferentes parámetros de la configuración de generación de texto, manipulando el tamaño de la frase. Estos parámetros serán utilizados en el producto final de generación de texto."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaRqKmE_lLAG"
      },
      "source": [
        "def generate_text(meme_id, n_memes = 1):\n",
        "  input_ids = tokenizer.encode(f\"<|{meme_id}|>\", return_tensors='tf')\n",
        "  generated_text_samples = model.generate(\n",
        "    input_ids, \n",
        "    max_length=40,  \n",
        "    num_return_sequences=n_memes,\n",
        "    no_repeat_ngram_size=2,\n",
        "    repetition_penalty=1.5,\n",
        "    top_p=0.92,\n",
        "    temperature=.85,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True\n",
        "  )\n",
        "  for i, beam in enumerate(generated_text_samples):\n",
        "    print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45aPkpm1mSyV",
        "outputId": "99622b00-df72-48a8-e30a-9117f95f74d5"
      },
      "source": [
        "generate_text('0004', 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: <|0004|>se cree nino y dice que | le importa la madre<0013||asi que eres un joven? | dimeque se siente no saber nada\n",
            "1: <|0004|>no | le gusta hacer nada en la vida el sabado no tiene otra excusa para que los demas te quieran comer<< |0011|pense  q\n",
            "2: <|0004|>va al america | se llama yara<<0011||]]le dije que no hariamos nada le dijo que me amaba pero era atea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0AEz4aLmdP3",
        "outputId": "b3b831df-6ad1-4fc0-c8cc-68f78e16e861"
      },
      "source": [
        "generate_text('0007', 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: <|0007|>no se si es una broma | o me esta ignorando<>y a justin bieber le gusta el chico que tiene bigote tu0013|\n",
            "1: <|0007|>no se si me esta ignorando | o se quiere decir que no quiere dar like a mi facebook<>¡sera feliz!!! no estoy seguro de ti\n",
            "2: <|0007|>no se si esta es un codigo | o solo piensa que quiere meter a la chica mas hermosa< | dime que se siente ser tan una de las mas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXS3Qrdunp1U"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}