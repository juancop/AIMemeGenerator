{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paquetes\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# Para eliminar palabrotas\n",
    "import spanlp\n",
    "#from spanlp.palabrota import Palabrota\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/daniel/Documents/GitHub/AIMemeGenerator/data/scrapper'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ruta de archivos a procesar\n",
    "path_original = '/home/daniel/Documents/GitHub/AIMemeGenerator'\n",
    "#path_original = os.getcwd()\n",
    "path = path_original + '/data/scrapper'\n",
    "os.chdir(path)\n",
    "os.getcwd()\n",
    "# ruta de directorio a crear\n",
    "#path_bases_no_tildes = path_original + \"/data/bases_no_tildes\"\n",
    "#os.makedirs(path_bases_no_tildes,0o007)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir()\n",
    "files = [val for val in files if re.search(r'.csv\\Z', val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [#['scrapymemes10m.csv',\n",
    "#'scrapymemes33m.csv',\n",
    "#'scrapymemes9m.csv',\n",
    "#'scrapymemes11m.csv',\n",
    "#'scrapymemes12m.csv',\n",
    "#'scrapymemes22m.csv',\n",
    "#'scrapymemes2mp2.csv',\n",
    "#'scrapymemes17m.csv',\n",
    "#'scrapymemes18m.csv',\n",
    "#'scrapymemes25m.csv',\n",
    "#'scrapymemes13m.csv',\n",
    "#'scrapymemes21m.csv',\n",
    "#'scrapymemes26m.csv',\n",
    "#'scrapymemes7m.csv',\n",
    "#'scrapymemes20m.csv',\n",
    "#'scrapymemes19m.csv',\n",
    "'scrapymemes2mp2.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scrapymemes2mp2.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: scrapymemes2mp2.csv\n",
      "Cantidad de registros: 205502\n",
      " \n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Parámetros: Campos para reemplazar tildes\n",
    "x = \"aaaaaaeeeeiiiioooooouuuuy\"       # Tildes\n",
    "y = \"àáâãäåèéêëìíîïðòóôõöùúûüý\"       # Caradteres que no fueron identificados por utf-8\n",
    "char_replace = {'Ã“':\"O\", 'Ã‘':'O','Ã­':'I',\n",
    "                'Ã‰':'E','Ãš':'U','â™':'u','âˆ':'e','ÃŒ':'I',\n",
    "                'Ãˆ':'E','Ãš':'U','â€':'A','Ã':'A','Ã±':'n','Ã‘':'ñ'}\n",
    "#files=  files[0]\n",
    "\n",
    "exceptions = [\"ano\",\"pendejo\",\"pendeja\",\"estupido\",\"estupida\",\"joder\",\"tonto\",\"tonta\",\"pete\",\"marinero\",\"bobo\",\n",
    "             \"perro\",\"idiotas\",\"idiota\",\"computadora\",\"joda\",\"dios\",\"jodas\",\"jode\",\"la computadora\",\n",
    "              \"imbecil\",\"pagan\",\"paja\",\"paga\",\"ojo\",\"internet\",\"rata\",\"baboso\",\"bochinche\",\"boletoso\",\n",
    "             \"bombril\",\"cachon\",\"guiso\",\"hoyo\",\"jartera\",\"lampara\",\"lamparoso\",\"lichigo\",\"mamera\",\"orto\",\n",
    "             \"tontarron\",\"cojones\",\"globalizacion\"]\n",
    "\n",
    "include_words = [\"verga\",\"culaso\",\"maricon\",\"marico\",\"putear\",\"putona\",\"puti\",\"putita\"]\n",
    "\n",
    "\n",
    "# groserias\n",
    "from spanlp.palabrota import Palabrota\n",
    "from spanlp.domain.strategies import CosineSimilarity\n",
    "from spanlp.domain.strategies import JaccardIndex\n",
    "\n",
    "# Limpieza de texto\n",
    "from spanlp.domain.strategies import Preprocessing, TextToLower, NumbersToVowelsInLowerCase ,NumbersToConsonantsInLowerCase, ExpandAbbreviations\n",
    "from spanlp.domain.countries import Country\n",
    "\n",
    "#strategies = [TextToLower(), NumbersToVowelsInLowerCase(), \n",
    "#             NumbersToConsonantsInLowerCase(), ExpandAbbreviations()] # Defino mis estrategias de limpieza o pre-procesamiento\n",
    "\n",
    "#cosine = CosineSimilarity(0.95, normalize=False) \n",
    "#jaccard = JaccardIndex(threshold=0.95, normalize=False, n_gram=2)\n",
    "palabrota = Palabrota(countries=[Country.COLOMBIA,Country.ESPANA, Country.ARGENTINA], exclude=exceptions, \n",
    "                       include=include_words) #,\n",
    "#                     distance_metric=cosine)\n",
    "\n",
    "\n",
    "# Objetos\n",
    "sin_convertir = {}     # Guardará índices de filas que no se pudieron convertir o de groserías.\n",
    "word_list     = []     # Listado de caracteres\n",
    "rows_to_drop  = [' Personalizado','plantilla']\n",
    "list_print    = np.arange(0,1000000,10000)\n",
    "\n",
    "for file in files:\n",
    "    print(\"Archivo:\",file)\n",
    "    file_name = re.sub('\\.csv$', '', file) \n",
    "    # Cargar archivos:\n",
    "    base = pd.read_csv(\"./\"+file,encoding=\"utf-8\")                            # Cargar archivo\n",
    "    base.drop_duplicates(subset=['plantilla','texto'],inplace=True,ignore_index=True) # Eliminar duplicados\n",
    "    base = base[~base.plantilla.isin(rows_to_drop)]\n",
    "    base[\"texto_no_tilde\"] = \"\"                                           # Crear campo de texto para almacenar tildes\n",
    "    base[\"palabrota\"]     = \"\"\n",
    "    id_no_encode = []                                                     # Lista para almacenar code_id     \n",
    "\n",
    "    print(\"Cantidad de registros:\",base.shape[0])\n",
    "    print(\" \")\n",
    "    index = 0            # índice en cero para iterar por filas\n",
    "    # Loop para convertir caracteres de latin-1 a utf-8 y guardar información en nueva tabla\n",
    "    for t in base.texto:\n",
    "    #for index in range(0,base.shape[0]):\n",
    "        if index in list_print:\n",
    "            print(index)\n",
    "        try:\n",
    "            #t = base.values[index,2]\n",
    "            #t = base.iat[index,2]\n",
    "            tab_map = t.maketrans(y, x)\n",
    "            t = t.translate(tab_map).strip()\n",
    "            t =  t.encode(\"latin-1\").decode(\"utf-8\")\n",
    "            for a,b in char_replace.items():\n",
    "                t = t.replace(a,b)\n",
    "            t = t.translate(tab_map).strip()\n",
    "            #base.values[index,3] = t\n",
    "            base.iat[index,3] = t\n",
    "            # Filtro de groserías\n",
    "            base.iat[index,4] = palabrota.contains_palabrota(t)\n",
    "            #base.values[index,4] = palabrota.contains_palabrota(t)\n",
    "            index += 1\n",
    "        # Si no logra convertir un caracter, guardar el índice dentro de la base y continuar.    \n",
    "        except UnicodeEncodeError:\n",
    "            id_no_encode.append(index)\n",
    "            index += 1\n",
    "       #     array = t.split() \n",
    "       #     for k in array:\n",
    "       #         try:\n",
    "       #             k.encode(\"latin-1\").decode(\"utf-8\")\n",
    "       #         except:\n",
    "       #             word_list.append(k)\n",
    "       #         pass\n",
    "       #     pass\n",
    "        except UnicodeDecodeError:\n",
    "            id_no_encode.append(index)\n",
    "            index += 1\n",
    "       #     array = t.split() \n",
    "       #     for k in array:\n",
    "       #         try:\n",
    "       #             k.encode(\"latin-1\").decode(\"utf-8\")\n",
    "       #         except:\n",
    "       #             word_list.append(k)\n",
    "       #         pass\n",
    "       #     pass        \n",
    "    # Guardar archivos limpios\n",
    "    base = base[base[\"texto_no_tilde\"] != \"\"]\n",
    "    #base = base[base[\"palabrota\"]==False]\n",
    "    base = base[base.palabrota == 0]\n",
    "    str_file = path_original + \"/data/bases_no_groserias/\" + file_name + \"_no_tilde.csv\"\n",
    "    base.to_csv(str_file, index=False )\n",
    "    del base, index, id_no_encode\n",
    "    # Guardar índices\n",
    "  #  sin_convertir[file_name] = id_no_encode\n",
    "   # print(\"registros sin limpieza:\",len(id_no_encode))\n",
    "    # print(\"Expresiones sin limpiar:\",len(word_list))\n",
    "    print(\" \")\n",
    "# word_list = list(dict.fromkeys(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scrapymemes33m_no_tilde.csv',\n",
       " 'scrapymemes7m_no_tilde.csv',\n",
       " 'scrapymemes16m_no_tilde.csv',\n",
       " 'scrapymemes32m_no_tilde.csv',\n",
       " 'scrapymemes20m_no_tilde.csv',\n",
       " 'scrapymemes6m_no_tilde.csv',\n",
       " 'scrapymemes1m_no_tilde.csv',\n",
       " 'scrapymemes24m_no_tilde.csv',\n",
       " 'scrapymemes10m_no_tilde.csv',\n",
       " 'scrapymemes9m_no_tilde.csv',\n",
       " 'scrapymemes5m_no_tilde.csv',\n",
       " 'scrapymemes8m_no_tilde.csv',\n",
       " 'scrapymemes28m_no_tilde.csv',\n",
       " 'scrapymemes21m_no_tilde.csv',\n",
       " 'scrapymemes19m_no_tilde.csv',\n",
       " 'scrapymemes15m_no_tilde.csv',\n",
       " 'scrapymemes17m_no_tilde.csv',\n",
       " 'scrapymemes22m_no_tilde.csv',\n",
       " 'scrapymemes11m_no_tilde.csv',\n",
       " 'scrapymemes18m_no_tilde.csv',\n",
       " 'scrapymemes23m_no_tilde.csv',\n",
       " 'scrapymemes25m_no_tilde.csv',\n",
       " 'scrapymemes31m_no_tilde.csv',\n",
       " 'scrapymemes13m_no_tilde.csv',\n",
       " 'scrapymemes30m_no_tilde.csv',\n",
       " 'scrapymemes29m_no_tilde.csv',\n",
       " 'scrapymemes14m_no_tilde.csv',\n",
       " 'scrapymemes12m_no_tilde.csv',\n",
       " 'scrapymemes4m_no_tilde.csv',\n",
       " 'scrapymemes3m_no_tilde.csv',\n",
       " 'scrapymemes26m_no_tilde.csv',\n",
       " 'scrapymemes27m_no_tilde.csv',\n",
       " 'scrapymemes2m_no_tilde.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(r'/home/daniel/Documents/GitHub/AIMemeGenerator/data/bases_no_groserias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
